---
title: "Statistical Modeling Workflow"
output:
  html_document:
    df_print: paged
---

### Introduction

The purpose of this notebook is to familiarize yourself with the proper workflow of creating and validating statistical models. For this exercise, we will use the built-in iris dataset, which is a dataset that provides measurements of various species of the iris flower.

<center>

![Iris sibirica](images/iris_photo.jpeg){width="300"}

</center>

First, load the tidyverse package. This will enable the libraries we will be using often, such as dplyr and others.

```{r}
library(tidyverse)
```

As mentioned previously, we will use the iris dataset. R has many built-in datasets that can be accessed by the `datasets` package. In our case, we will turn the iris dataset into a tibble as well, which enables us to explore the data easier and increase performance. In cases where you load a csv via the `read_csv()` function of the `readr` package, loading into a tibble first is not necessary.

```{r}
iris_df <- tibble(iris)
```

### Data Exploration

Let's take a look and familiarize ourselves with our data. This step is important as the type of analysis we can do is informed by the type of data we have.

```{r}
iris_df
```

As you can see from above, the tibble allows us to see the first 10 rows of the dataset, which is enough to get a feel of the data we are dealing with. We note that the `Sepal.Length`, `Sepal.Width`, and `Petal.Length` columns have a double data type (numerical and continuous), while the Species column has the factor data type (categorical variable).

#### Generate Summary Statistics

Let's explore each column further and see how data is distributed. Understanding the distribution of data is important because it has implications on what type of analysis we can do. For example, parametric tests such as ANOVAs, t-tests, etc... have assumptions that needs to be true in order for the analysis to be valid.

```{r}
summary(iris_df)
```

Quickly glancing at the summary statistics of each column, there doesn't seem to be any obvious outliers in the dataset as far as the numerical columns go. Looking at species, we see that we have three categories present in the dataset and there are equal numbers of each category.

#### Look at Data Distribution

Let's graph some histograms of the numerical columns to visualize their underlying data distributions. We won't create a histogram of the `Species` column because it is not a numerical column.

```{r}
for (i in colnames(iris_df %>% select(!Species))) {
  hist(iris_df[[i]], main=paste("Histogram of ", i), xlab = i)
}
```

Looking at each graph, we observe that only `Sepal.Width` has a "normal" distribution. The `Sepal.Length` column is kind of normal, but the last two columns are certainly right-skewed. Additionally, take note that the width and length size units are different in magnitude, although the magnitude is not too extreme.

Finally we need to take a look at if we have missing values in our dataset.

```{r}
map(iris_df, ~sum(is.na(.)))
```

There are no missing values in our dataset, which means we don't need to perform imputation. Imputation is the process of filling in null values. There are multiple ways to impute data. First, we can take the mean of the column and use that to fill in on the missing data. Second, we can fill values based on the value of the previous or next row. There are also advanced imputation techniques, such as knn imputation.

We are now finished looking at our dataset broadly. Based on our exploration, we decided to keep all the columns. The next step is in preprocessing our dataset. Preprocessing is the step before model creation where we perform standardization or create "dummy variables" for our feature columns.

### Data Preprocessing

Standardization is the process of transforming our numerical values into z-scores. In cases of columns where there are more than two categories, we need to create dummy variables. A dummy variable is a bivariate variable that takes either 0 or 1 (observed or not observed) as its value. Creating dummy variables allow us to analyze categorical features easily and incorporate them in models.

Let's start by standardizing our numerical columns. Note that it is best practice to NOT replace the original columns, but instead create new columns for the scaled columns. When creating columns, it is also best practice to use `snake_case`.

```{r}
iris_df <- iris_df %>%
  mutate(sepal_length_scaled = scale(Sepal.Length),
         sepal_width_scaled = scale(Sepal.Width),
         petal_length_scaled = scale(Petal.Length),
         petal_width_scaled = scale(Petal.Width))
```

Now let's create dummy variables for the `Species` column. When we create dummy variables (or dummy columns), it is advisable to keep the original column's name as a prefix.

```{r}
library(fastDummies)
```

```{r}
iris_df <- iris_df %>% dummy_cols(select_columns="Species")
```

Let's view the final dataset.

```{r}
iris_df
```

Looking at the dataset above, we see that we now have 12 columns as compared to 5 in the original dataset. Click the forward facing arrow by the column names to explore the rest of the columns we have created. With this, our pre-processing step is complete.

### Splitting Data Into Training and Test Data

The next step is to divide our dataset into training and testing datasets. We divide our dataset into training and testing datasets in order to have the opportunity to fine tune our model (using the training set), and then measure its performance (using the test set). Doing this allow us to reduce look-ahead bias. Having a test set allows us to evaluate the performance of our model using data it wasn't trained on. Generally, a good split is 80% of the original dataset used as the training set and 20% as the test set. We will do simple random sampling to allocate our data into the two sets.

```{r}
set.seed(420) #random state to ensure reproducability
train <- slice_sample(iris_df, prop=0.80)
test  <- anti_join(iris_df, train)
```

### Specifying Validation Method

We now finally turn to creating our statistical model. Before implementing our model, we need to first specify how we will validate our model's performance. One of the most common techniques for model validation is k-fold cross validation. What this does is it partitions a dataset into k groups. From each group, it creates both a training and a testing set to evaluate model performance. The number of folds can be anything, but it's advisable to keep the number of folds to 5 or 10. The reason being is that while increasing the number of folds may increase model accuracy, it will also increase the error variance of the model. The main purpose of k-fold cross validation is not only to measure model performance, but also measure how variable this performance is.

Performing cross validation in R can be achieved by using the `caret` package.

```{r}
library(caret)
```

Let's specify the cross validation method with 10 folds.

```{r}
ctrl <- trainControl(method="cv", number=10)
```

### Creating The Logistic Regression Model

To create a model, we need to identify our target column (dependent variable) and our feature columns (independent variables). Let's create a simple logistic regression model (a model where the dependent variable is bivariate) to predict whether a flower is of the species "setosa." Of course, we have other supervised machine learning techniques to handle classification tasks (k-nearest neighbors clustering, for example), but we will just focus on the logistic regression example for now.

There are multiple ways to create the logistic regression model. You can use either the equation form or just divide the training set into the feature columns and isolate the target column.

```{r}
X_train <- train %>% select(sepal_length_scaled, sepal_width_scaled, petal_width_scaled, petal_length_scaled)
y_train <- train[["Species_setosa"]]
```

Note that we use the `data.frame()` function because the `carett::train()` function cannot set row names on a tibble.

```{r}
log_model <- train(x=data.frame(X_train), y=factor(y_train), trControl=ctrl, method="bayesglm")
print(log_model)
```

Our statistical model trained on the training data achieved an accuracy score 100% on cross-validation. What this means is that using flower measurements alone can help us classify the iris species. Since this model looks good, we don't need to further refine it or tune any other parameters (since there are no hyperparameters on this particular model).

Let's also explore the model coefficients of the final model.

```{r}
coef(log_model$finalModel)
```

In a logistic regression, the coefficients represent the average change in log odds in response to the dependent variable. To convert this in odds ratios, we can exponentiate each coefficients.

```{r}
exp(coef(log_model$finalModel))
```

Finally, we can look to see which variables are significant for our model. Typically, we want to create the most parsimonious (simplest) model by only including the most significant variables. We do this either through forward or backward selection. Either way is fine, but I prefer backward selection. This means I start with the full model and then simplify.

```{r}
summary(log_model)
```

In this case, it looks like `petal_length_scaled` is the only statistically significant variable. Let's try creating another model excluding `sepal_length_scaled`, since that variable has the highest p-value.

```{r}
log_model_2 <- train(x=data.frame(X_train %>% select(!sepal_length_scaled)), y=factor(y_train), trControl=ctrl, method="bayesglm")
summary(log_model_2)
```

As it stands, the accuracy (correctly classifying each observation) of our model using the training set is 100%. One thing to pay attention to as well is the change in estimate after removing variables. If removing a variable causes a 10% change in the estimate/coefficient (in either direction), keep the variable as it is most likely a confounding variable. In this case, removing `sepal_length_scaled` did not affect any of our coefficients. Let's keep going.

```{r}
log_model_3 <- train(x=data.frame(X_train %>% select(!c(sepal_length_scaled, petal_width_scaled))), y=factor(y_train), trControl=ctrl, method="bayesglm")
summary(log_model_3)
```

```{r}
log_model_4 <- train(x=data.frame(X_train %>% select(petal_length_scaled)), y=factor(y_train), trControl=ctrl, method="bayesglm")
summary(log_model_4)
```

Having a model with only 1 independent variable, we also reduced our AIC score (this score measures the error in our model: lower is better). But does this model perform as well as our first model?

```{r}
log_model_4
```

Surprisingly, the accuracy is 100% still. Our model is parsimonious and it looks like we can predict whether an iris is setosa or not by petal length alone! Let's go with this as our best model.

### Measuring Model Performance On New Data

The final test is using our model to predict values using data that it hasn't seen before. Let's now determine the model's accuracy using the testing set. First, let's split the feature and target columns, similar to what we did in the training set. Note that we do cross validation when we are tuning our models. However, once we are happy with our model, we don't need to perform cross validation on our test set.

```{r}
X_test <- test %>% select(sepal_length_scaled, sepal_width_scaled, petal_width_scaled, petal_length_scaled)
y_test <- test[['Species_setosa']]
```

Then, let's generate predictions using our logistic regression model.

```{r}
y_pred <- predict(log_model_4, (X_test %>% select(petal_length_scaled)))
```

Finally, let's measure the accuracy of our model.

```{r}
confusionMatrix(factor(y_pred),factor(y_test))
```

So why exactly is this model good? Let's try graphing the `petal_length` with `species` from the original dataset.

```{r}
iris_df %>% ggplot(aes(x=Species, y=petal_length_scaled, fill=Species)) + geom_violin() + labs(main="Petal Length by Iris Species", y="Petal Length Scaled") + theme_minimal()
```

As it turns out, it just so happens that there is a distinct separation of setosa from the two other species when it comes to petal length. Looking back at our histogram of `petal_length` you could even see this pattern as well. It is true that we could have predicted the outcome had we made charts like these (which I recommend doing in the beginning), but I wanted to mainly highlight the general process of creating logistic regressions. If we had used a different species of iris, our logistic regression would not be as "good."

### Conclusion

The above results states that our model was able to predict with 100% accuracy on the test set. What this means is that as long as we have petal length, we can be confident in classifying whether an iris is "setosa" or not.

This is the general process of creating statistical models properly. While knowing statistical modeling is important, it is also equally as important to understand the proper process of exploring your data, creating your models, and evaluating them. The entire process outlined will allow you to create useful models that will work in production. Some models may look great but perform poorly once they receive new data simply because the proper validation steps were not followed.

Note that there are two main reasons why we create statistical models: to describe our data or to predict.

### Future Directions

Try creating a logistic regression model using the other iris species and see how well your logistic regression performs.
